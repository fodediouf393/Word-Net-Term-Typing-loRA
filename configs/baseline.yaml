model:
  name: "bert-base-uncased"
  num_labels: 4

lora:
  enabled: false

train:
  output_dir: "outputs/runs/baseline"
  run_name: "baseline-bert"
  seed: 42

  epochs: 3
  batch_size: 32
  eval_batch_size: 64
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.06

  fp16: true
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0

  logging_steps: 50
  eval_steps: 200
  save_steps: 200

  metric_for_best_model: "macro_f1"
  greater_is_better: true
  load_best_model_at_end: true
